### 1.2.3 Orders of Growth

The previeous example illustrates that the processes can differ considerably in the rates at which they consume computational resources. One convenient way to describle this difference is to use the notion of *order of growth* to obtain a gross measure of the resources requried by a process as the inputs become larger.

Let `n` be a parameter that measure the size of the problem, and let `R(n)` be the amount of resources the process requires for a problem of size `n`.
In our previous examples we took `n` to be the number for which a given function is to be computed, but there are other possibilites.
For instance, if our goal is to compute an approximation to the square root of a number, we might take `n` to be the number of digits accuracy required.
For matricx multiplication we might take `n` to be the number of rows in the matrices.
In general there are number of porperties of the problem with respect to which it will be desirable to analyze of a given process.
Similarly, `R(n)` might measure the number of internal storage registers used, the number of elementry machine operations performed, and so on.
In computers that do only a fixed number of operations at a time, the time required will be proportional to the number of elementary machine operations performed.

We say `R(n)` has order of growth `T(f(n))`, written `R(n) = T(f(n))` (pronounced "theta of f(n)"), if there are positive constants `k1` and `k2` independent of `n` such that `k1*f(n) <= R(n) <= k2*f(n)` for any sufficently large value of `n`. (In ofhter works, for large `n`, the value of R(n) is sandwiched between `k1f(n)` and `k2*f(n)`.)