### 1.2.3 Orders of Growth

The previeous example illustrates that the processes can differ considerably in the rates at which they consume computational resources. One convenient way to describle this difference is to use the notion of *order of growth* to obtain a gross measure of the resources requried by a process as the inputs become larger.

Let `n` be a parameter that measure the size of the problem, and let `R(n)` be the amount of resources the process requires for a problem of size `n`.
In our previous examples we took `n` to be the number for which a given function is to be computed, but there are other possibilites.
For instance, if our goal is to compute an approximation to the square root of a number, we might take `n` to be the number of digits accuracy required.
For matricx multiplication we might take `n` to be the number of rows in the matrices.
In general there are number of porperties of the problem with respect to which it will be desirable to analyze of a given process.
Similarly, `R(n)` might measure the number of internal storage registers used, the number of elementry machine operations performed, and so on.
In computers that do only a fixed number of operations at a time, the time required will be proportional to the number of elementary machine operations performed.

We say `R(n)` has order of growth `T(f(n))`, written `R(n) = T(f(n))` (pronounced "theta of f(n)"), if there are positive constants `k1` and `k2` independent of `n` such that `k1*f(n) <= R(n) <= k2*f(n)` for any sufficently large value of `n`. (In ofhter works, for large `n`, the value of R(n) is sandwiched between `k1f(n)` and `k2*f(n)`.)

For instance, with teh linear recursive process for computing factorial described in [Section 1.2.1](1.2.1_linear_recursion_and_iteration.md) the number of steps grows proportionally to the input `n`. Thus, the steps required for this process grows as `T(n)`. We also saw that sace required grows as `T(n)`. For the iterative factorial, the number of steps is still `T(n)` but the space is `T(1)` - that is constant. *(36)*. Trhe tree-recursive Fibonacce computation requires `T(fi**n)` stets ans space `T(n)`, where `fi` is the bolden radio described in [Section 1.2.2](1.2.2_tree_recursion.md)

> *(36)* These statements mask a great deal of oversimplification. For instance, if we count process steps as "machine operations" we are making the assumption that the number of machine operations neede to perform, say, a multiplication is independent of the size of numbers to be mutiplied, which is false if the numbers are sufficiently large. Similar revars hold for the estimates of space. Like the desing and description of a porcess, the analysis of a process can be carried out at various levels of abstraction.


Order of growth provide only a crude description of the behaviour of a process. For example, a process requireding `n**2` steps and a process requiiring `1000*n**2` steps and a preocess requiring `3n**2 + 10*n + 17` steps all have `T(n**2)` order of growth. On the other hand, ofret of gowth provides a useful indecation of how we may expect the behfviour of porcess to change as we change the size of the problem. For a `T(n)` (linear) prcess, doubling the size will roughly double the amount of resources used. For expnential process, each increment in problem size will multiply the resource utilization by a constant factor. in the remainder of Section 1.2 we will examine two altorithms whose order of growth is logarithmic, so that doublin ghe problem size icreases the resource requirement by a constant amount.